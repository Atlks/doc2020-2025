大数据量下的数据库分区算法

按照体积来分区，每次新建数据获取最后分区文件的时候 ，判断下文件体积，如果过大，那么new 新的分区文件即可。。


获取最后分区文件的时候 ，判断下文件体积Insert

function pdo_insertV4(rcd, collName) {

   log_enterFun(arguments)

   let lastPrtn = getPartnsLast(collName)

   let db_conn_dt = pdo_connV3(lastPrtn.dbf)

   if (typeof rcd == "string")
       rcd = JSON.parse(rcd)
   db_conn_dt.push(rcd)

   pdo_save(db_conn_dt, lastPrtn.dbf)
   return db_conn_dt;
}


function getPartnsLast(collName) {

   let partList = getPartns(collName)

   let lastptn = partList[partList.length - 1];


   let dbfile = lastptn.dbf
   if (filesize(dbfile) > 1024 * 1024) {
       // if fi如果lesieze > 1M 物 ，new file

       lastptn = newprtn(collName)
   }
   return lastptn;
}


查询的时候，多分区连接join 数组融合合并即可
function pdo_queryV2(qryDsl, collName) {

   log_enterFun(arguments)
   // var  _ = await import("lodash")  not err,but cant use _.flt said cant find this fun
   // import  lds = require('lodash') ;
   console.log("::23")
   let data;
   var {readFileSync, writeFileSync, appendFileSync} = require("fs");

   let rzt_all = [];

   let ptns = getPartns(collName)
   for (let pt of ptns) {

       let dbfile =pt.dbf

       var txt = readFileSync(dbfile).toString();
     //  console.log(" dbtxt len100 =>" + txt.substring(0, 100))
       data = JSON.parse(txt)

       require("esm-hook");
       //  const _ = require('lodash').default
       const _ = require('lodash')
       let rzt = _.filter(data, qryDsl)
       rzt_all = array_merge(rzt_all, rzt)
   }


   console.log("[pdo_query]rzt is=>" + JSON.stringify(rzt_all).substring(0, 300))
   return rzt_all;

}

Update 于del
按照id查找到记录，超找到记录所在分区，更新分区数据

/**
*
* @param data2
* @param file2
*/
function pdo_saveV2(data2, collName) {


   let qryDsl={"id":data2.id}
   let rowsAll;
   var {readFileSync, writeFileSync, appendFileSync} = require("fs");

   let rzt_all = [];

   let ptns = getPartns(collName)
   for (let pt of ptns) {

       let dbfile = pt.dbf

       var txt = readFileSync(dbfile).toString();
       //  console.log(" dbtxt len100 =>" + txt.substring(0, 100))
       rowsAll = JSON.parse(txt)

       require("esm-hook");
       //  const _ = require('lodash').default
       const _ = require('lodash')
       let rows = _.filter(rowsAll, qryDsl)
       if(rows)
       {
           let obj=rows[0]
           copyProp(data2,obj)
           require("./enc")
           writeFileSyncx(pt.dbf, json_encode(rowsAll));
           return
       }

   }




}

分区的三大操作
新建分区 

/**
*
* @param collName
* @returns {{dbf: string, ptn: string}}
*/
function newprtn(collName) {

   let dbdir = "../db/111356/"
   let ptnname = collName + "_" + idBasetime() + ".json";
   let dbfile = dbdir + ptnname

   return {"ptn": ptnname, "dbf": dbfile};
}

获取分区列表
/**
*
* @param collName
* @returns {string[]}
*/
function getPartns(collName) {
   let dbdir = "../db/111356/"
   var {readFileSync, writeFileSync, appendFileSync} = require("fs");
   var fs = require("fs");
   let partList = fs.readdirSync(dbdir);
   partList = partList.filter(f => {
       return f.startsWith(collName)
   })
   if (partList.length == 0) {
       let dbfile = dbdir + collName + ".json"
       writeFileSync(dbfile, "[]")
       partList = [collName + ".json"]
   }
   let arr = []
   for (let ptnname of partList) {
       let dbfile = dbdir + ptnname
       let ptn = {"ptn": ptnname, "dbf": dbfile};
       arr.push(ptn)
   }
   return arr;
}


获取最新分区  ，，准备操作
/**
*
* @param collName
* @returns {string}
*/
function getPartnsLast(collName) {

   let partList = getPartns(collName)

   let lastptn = partList[partList.length - 1];


   let dbfile = lastptn.dbf
   if (filesize(dbfile) > 1024 * 1024) {
       // if fi如果lesieze > 1M 物 ，new file

       lastptn = newprtn(collName)
   }
   return lastptn;
}

