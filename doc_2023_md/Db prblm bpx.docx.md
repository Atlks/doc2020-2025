Db prblm bpx


数据格式容易损坏问题，换行来append

数据多行多进程同时读写的
使用、r换行来append，忽略不值钱的行即可
不要直接一个大json文件

支对表的元数据的管理和变更操作，我们从原有的 csv 文件进化出了第一个特性，分离元数据和数据的存储。
添加，修改或者删除  appoend only
讨论完了元数据的管理，我们再来看 CSV 文件对于其他常见的数据库操作还有什么做得不够好的。除了最频繁的查询语句，另一类常见的操作就是添加，修改或者删除表里的数据。对于添加，我们只需要将新数据添加到文件的末尾。对于修改，如改变某一行的某一个属性或删除某一行，就需要在数据文件中进行修改。相较于在文件末尾添加，文件中的修改会低效很多，尤其是当数据文件特别大的时候。

有什么思路来改进呢？那些数据库先贤就想了一招，分开管理的思路也可以用在数据本身呢。设想一



如果要执行更新语句呢，比如把姓名为"Braund, Mr. Owen Harris"年龄纪录更新成 37 岁，这又应该怎么操作呢？我们可以在数据文件中添加一行新数据(第 9 行)，这行数据复制了第一行但是把年龄改成 37。

然后在读取数据的时候，先读取 slot_table，然后逆序读取所有行的标注信息(读取到 2D 后就忽略第二行)，就能知道哪些行是有效的，哪些行可以略过不用读取了。

或者正确读取行，但是只取最后的新行数据。。



 vacuum 操作(或者叫 compact)
对于数据的增删改，我们已经可以对数据文件和 slot_table 都实现 append_only。还有什么问题吗？对于一个数据表，每次操作都会添加新信息，久而久之，数据文件越来越大，而且无效的行越来越多，岂不是很浪费空间。有什么办法可以优化呢？有。数据库都会支持 vacuum 操作(或者叫 compact)，这个操作所做的就是读取数据文件和 slot_table，然后根据标注把有效的数据行写回新的文件。比如，对我们的示例进行 vacuum 操作，新的数据文件和 slot_table 如下所示:



列存(column-oriented store)
实体表不仅有大量数据行，属性也很多(100 到 200 都很常见)。可是，大部分的分析报表语句仅需要读取相关的几个属性(列)。为了运行该语句，就需要把整个实体表的数据读到内存中来抽取需要的属性列。设想一个实体表有 100 个属性，10 亿条数据，但某个语句只需要用到 3 个属性。按照 CSV 方式读取数据，97%的数据是没有用处的。贪得无厌的数据库的大牛想，有什么办法可以优化需要读取的数据吗？于是列存(column-oriented store)就这样出现了。

类似于 CSV 这样把每一个 tuple 的数据存放在一起的存储方式叫行存(row-oriented store)。相对应的列存，就是指把一个表的每个属性， 单独存在一个数据文件中。还是沿用上面 titanic 的例子，我们会有单独的数据文件(还有 slot_table 文件)来存储姓名，船票价格，登船码头，等等。在读取的时候，根据查询语句需求，需要用到哪个属性就读取哪个属性的数据文件。按照前面的例子，我们只需要读取原来的 3 个属性的数据文件，读取速度自然就提高了。


们从最原始的使用 CSV 文件格式来存储数据，一步一步根据数据库的操作需要，"进化"出了下面这些优化方法:


为了更好得支持对表的元数据的管理和变更操作, 分离元数据和数据的存储


为了更高效地实现增删改数据，引入 slot_table 以及标注信息来纪录对数据的增删改，并且引入 vacuum 操作定期清理无用的行数据


为了更高效得存储数据，用 byte 来存储数据配合高效的编码和解码算法来加速读取和写入


为了应对数据仓库中复杂报表的查询语句和超大量的数据读取，引入列存概念，并且用压缩算法来进一步优化数据量


多列索引
最后留个坑，虽然列存的实现，使得我们不用读取无用列的数据，但针对某些点查询的语句(point query)，比如"select col2 from table1 where col1 = 10", 我们依然需要读取 col1 和 col2 两列的全部数据。有什么办法可以优化这类查询吗？

下一篇，我们接着聊这类优化-索引(indexing)。


